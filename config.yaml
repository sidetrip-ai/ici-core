# Core System Configuration
system:
  loggers:
    structured_logger:
      console_output: true
      level: WARNING
      log_file: ./logs/structured_logger.log
      name: structured_logger
      source_token: $SOURCE_TOKEN
      host: $INGESTION_HOST
      use_betterstack: false
  state_manager:
    db_path: ./db/sql/ingestor_state.db
  validator:
    allowed_sources:
      - COMMAND_LINE
    rules: []
  chat_history_manager:
    json:
      base_path: ./db/chat
      use_subdirectories: true
      file_permissions: 0o600
      default_message_limit: 20
      max_messages_per_chat: 1000
  user_id_generator:
    default:
      sources:
        - cli
        - web
        - api
        - test
      default_identifier: anonymous
  external_services:
    perplexity:
      api_key: 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
      model: sonar
      request_timeout: 60
    travel_planner:
      context_window_days: 7
      max_chat_messages: 100
      chat_history_dir: "db/telegram_chats"

# Main Orchestrator Configuration
orchestrator:
  # Core orchestrator settings
  error_messages:
    generation_failed: I'm having trouble generating a response right now. Please try again later.
    no_documents: I don't have specific information on that topic yet.
    validation_failed: I'm unable to process your request due to security restrictions.
  generation_options:
    max_tokens: 1024
    temperature: 0.7
  num_results: 5
  similarity_threshold: 0.7
  rules_source: config
  user_context:
    default:
      permission_level: user
  validation_rules:
    default: []
  
  # Use travel-aware orchestrator
  type: travel_orchestrator
  
  # Components used by orchestrator
  generator:
    api_key: $GENERATOR_API_KEY
    base_url: http://localhost:11434
    base_retry_delay: 1
    chain_type: simple
    default_options:
      frequency_penalty: 0.0
      max_tokens: 1024
      presence_penalty: 0.0
      temperature: 0.7
      top_p: 1.0
    max_retries: 3
    memory:
      k: 5
      type: buffer
    model: moonshotai/kimi-vl-a3b-thinking:free
    provider: openrouter
    type: langchain
  
  embedder:
    sentence_transformer:
      batch_size: 32
      device: cpu
      model_name: all-MiniLM-L6-v2
  
  vector_store:
    chroma:
      type: chroma
      collection_name: messages
      embedding_function: sentence_transformer
      persist_directory: ./db/vector/chroma_db
  
  prompt_builder:
    error_template: 'Unable to process: {error}'
    fallback_template: 'Answer based on general knowledge: {question}'
    template: "Context:\n{context}\n\nQuestion: {question}\n\nYour response should be conversational and helpful. Remember to continue from any previous conversation if relevant."
  
  # Pipeline configurations
  pipelines:
    telegram:
      schedule:
        interval_minutes: 1
      ingestor:
        telegram:
          # Telegram API credentials
          api_hash: $TELEGRAM_API_HASH
          api_id: $TELEGRAM_API_ID
          phone_number: $TELEGRAM_PHONE_NUMBER
          session_string: $TELEGRAM_SESSION_STRING
          
          # Chat and message limits
          max_chats: 100            # Maximum number of chats to fetch (-1 for all available)
          max_messages_per_chat: 200  # Maximum messages per chat (-1 for all available)
          ignored_chats: []         # List of chat IDs to exclude from fetching
          
          # Rate limiting and batching settings
          batch_size: 50            # Number of messages to fetch in each request
          request_delay: 1.0        # Delay between batched requests in seconds
      preprocessor:
        chunk_size: 512
        include_overlap: true
        max_messages_per_chunk: 10
        time_window_minutes: 15
        store_chat_history: true
        chat_history_dir: "db/telegram_chats"

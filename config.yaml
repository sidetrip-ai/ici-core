# Core System Configuration
system:
  loggers:
    structured_logger:
      console_output: true
      level: WARNING
      log_file: ./logs/structured_logger.log
      name: structured_logger
      source_token: $SOURCE_TOKEN
      host: $INGESTION_HOST
      use_betterstack: false
  state_manager:
    db_path: ./db/sql/ingestor_state.db
  validator:
    allowed_sources:
      - COMMAND_LINE
    rules: []
  chat_history_manager:
    json:
      base_path: ./db/chat
      use_subdirectories: true
      file_permissions: 0o600
      default_message_limit: 20
      max_messages_per_chat: 1000
  user_id_generator:
    default:
      sources:
        - cli
        - web
        - api
        - test
      default_identifier: anonymous

# Main Orchestrator Configuration
orchestrator:
  # Core orchestrator settings
  error_messages:
    generation_failed: I'm having trouble generating a response right now. Please try again later.
    no_documents: I don't have specific information on that topic yet.
    validation_failed: I'm unable to process your request due to security restrictions.
  generation_options:
    max_tokens: 1024
    temperature: 0.7
  num_results: 5
  similarity_threshold: 0.7
  rules_source: config
  user_context:
    default:
      permission_level: user
  validation_rules:
    default: []
  
  # Components used by orchestrator
  generator:
    api_key: XXXXX-37908a969e7e0a71f8334ea88939ebb1d6f
    base_url: http://localhost:11434
    base_retry_delay: 1
    chain_type: simple
    default_options:
      frequency_penalty: 0.0
      max_tokens: 1024
      presence_penalty: 0.0
      temperature: 0.7
      top_p: 1.0
    max_retries: 3
    memory:
      k: 5
      type: buffer
    model: openrouter/optimus-alpha
    provider: openrouter
    type: langchain
  
  embedder:
    sentence_transformer:
      batch_size: 32
      device: cpu
      model_name: all-MiniLM-L6-v2
  
  vector_store:
    chroma:
      type: chroma
      collection_name: messages
      embedding_function: sentence_transformer
      persist_directory: ./db/vector/chroma_db
  
  prompt_builder:
    error_template: 'Unable to process: {error}'
    fallback_template: 'Answer based on general knowledge: {question}'
    template: "Context:\n{context}\n\nQuestion: {question}\n\nYour response should be conversational and helpful. Remember to continue from any previous conversation if relevant."
  
  # Pipeline configurations
  pipelines:
    github:
      schedule:
        interval_minutes: 60  # Check for new data every hour
      ingestor:
        github:
          data_file: "db/github_logs/github_chats.json"
          batch_size: 50
          max_repositories: 100  # Maximum number of repositories to process
          max_issues_per_repo: 200  # Maximum issues per repository
          max_prs_per_repo: 200  # Maximum pull requests per repository
          max_comments_per_item: 100  # Maximum comments per issue/PR
      preprocessor:
        github:
          chunk_size: 512
          include_overlap: true
          max_items_per_chunk: 10
          time_window_minutes: 60
          store_history: true
          history_dir: "db/github_history"
    
    generator:
      model: llama3
      provider: ollama
      type: langchain
      default_options:
        temperature: 0.7
        max_tokens: 1024